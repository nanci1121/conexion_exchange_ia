# Modelos LLM - Llama 3.2 3B GGUF

## üöÄ TL;DR - Funcionamiento Autom√°tico

**No tienes que hacer nada manualmente.** El modelo se descarga autom√°ticamente cuando ejecutas:

```bash
docker-compose up --build
```

El `Dockerfile` contiene instrucciones para:
1. Crear esta carpeta `/app/models` dentro del contenedor
2. Descargar autom√°ticamente `llama-3.2-3b-instruct-q4_k_m.gguf` desde HuggingFace
3. Montarla en `./llm_service/models` en tu m√°quina (host)

## üì¶ Lo que sucede autom√°ticamente

### Durante `docker-compose up --build`:

```dockerfile
# Dockerfile descarga y monta el modelo:
RUN mkdir -p /app/models
RUN wget -O /app/models/llama-3.2-3b-instruct-q4_k_m.gguf \
    https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
```

### Volumen compartido:

```yaml
# docker-compose.yml:
volumes:
  - ./llm_service/models:/app/models  # Host ‚Üî Contenedor
```

## ‚öôÔ∏è Estructura de carpetas

```
llm_service/
‚îú‚îÄ‚îÄ Dockerfile                          # Descarga el modelo autom√°ticamente
‚îú‚îÄ‚îÄ app.py                              # API FastAPI + llama.cpp
‚îú‚îÄ‚îÄ requirements.txt                    # Dependencias (llama-cpp-python)
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ README.md                       # Este archivo
    ‚îî‚îÄ‚îÄ llama-3.2-3b-instruct-q4_k_m.gguf  # SE DESCARGA AUTOM√ÅTICAMENTE (~2.5GB)
```

## üì• Primera ejecuci√≥n

### Paso 1: Clonar repositorio
```bash
git clone https://github.com/nanci1121/conexion_exchange_ia.git
cd conexion_exchange_ia
```

### Paso 2: Construir con Docker
```bash
docker-compose up --build
```

**Esto har√°:**
- ‚úÖ Construir imagen `email_ai_llm`
- ‚úÖ Descargar `llama-3.2-3b-instruct-q4_k_m.gguf` (~2.5GB) - TARDA 10-30 MIN
- ‚úÖ Montar en `./llm_service/models/`
- ‚úÖ Iniciar servicio en puerto 8000

**Tiempo estimado:**
- Primera vez: 10-30 minutos (descarga + construcci√≥n)
- Siguientes: 1-2 minutos (usa imagen cacheada)

### Paso 3: Verificar que funciona
```bash
curl http://localhost:8000/health
```

Respuesta esperada:
```json
{
  "status": "ok",
  "technology": "GGUF/llama.cpp",
  "model": "TinyLlama-1.1B"
}
```

## üîÑ Despu√©s de la primera descarga

El archivo `.gguf` estar√° en tu disco en:

**Windows:**
```
d:\02_DESARROLLO\Entrenar_IA_correos\llm_service\models\llama-3.2-3b-instruct-q4_k_m.gguf
```

**Linux/Mac:**
```
./llm_service/models/llama-3.2-3b-instruct-q4_k_m.gguf
```

### Comportamiento del volumen:

- ‚úÖ **Primera vez**: Se descarga dentro del contenedor ‚Üí se copia al host
- ‚úÖ **Siguientes veces**: El contenedor reutiliza el archivo del host (m√°s r√°pido)
- ‚úÖ **Persistencia**: El archivo se mantiene entre reinicios

## üìä Especificaciones del modelo

| Propiedad | Valor |
|-----------|-------|
| **Nombre** | Llama 3.2 3B Instruct |
| **Tama√±o** | ~2.5GB (ya cuantizado en Q4_K_M) |
| **Contexto** | 128K tokens (configurado a 4K en app.py) |
| **Cuantizaci√≥n** | Q4_K_M (4-bit, excelente balance precisi√≥n/velocidad) |
| **Precisi√≥n** | ~95-98% vs modelo original fp32 |
| **Velocidad CPU** | ~10-15 tokens/segundo (4 threads) |
| **Velocidad GPU** | ~100-150 tokens/segundo (si dispones de NVIDIA) |
| **RAM requerida** | 8GB m√≠nimo (16GB recomendado) |
| **Descarga** | <https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF> |

## üÜò Troubleshooting

### "Descarga muy lenta"

1. **Verifica tu conexi√≥n**: Download desde HuggingFace tarda m√°s en ciertos horarios
2. **Espera**: Primera descarga puede tomar 20-30 minutos con conexi√≥n lenta
3. **Reintentar**: Si se corta, `docker-compose up --build` retoma desde donde qued√≥

### "Model not found" o error en app.py

```bash
# Verifica que existe:
ls -la llm_service/models/llama-3.2-3b-instruct-q4_k_m.gguf

# Si no existe o est√° incompleto, limpia y reconstruye:
docker-compose down
rm -rf llm_service/models/*.gguf
docker-compose up --build
```

### "Out of memory" durante descarga

Si tienes <8GB RAM libre:
```bash
# Libera RAM cerrando otras aplicaciones
# Luego reintenta:
docker-compose up --build
```

### "Build fallido en wget"

Si HuggingFace tiene tiempo de espera:
1. Intenta de nuevo: `docker-compose up --build`
2. Usa mirror alternativo en Dockerfile:
   ```dockerfile
   RUN wget -O /app/models/llama-3.2-3b-instruct-q4_k_m.gguf \
       https://huggingface.co/TheBloke/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
   ```

## üöÄ GPU Support (Opcional)

Si tienes NVIDIA GPU, descomenta en `docker-compose.yml`:

```yaml
llm_service:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

Luego reconstruye:
```bash
docker-compose up --build
```

Ganancia de velocidad: **~50-100x m√°s r√°pido** que CPU.

## üìù Nota sobre .gitignore

El archivo `.gguf` **NO** debe commitarse al repositorio:

```gitignore
# Archivo generado/descargado autom√°ticamente
llm_service/models/*.gguf
```

Ya est√° configurado en el `.gitignore` del proyecto.

## ‚úÖ Checklist de despliegue

- [ ] Clonaste el repositorio
- [ ] Tienes Docker Desktop con WSL2 (Windows) o Docker Engine (Linux/Mac)
- [ ] Ejecutaste `docker-compose up --build`
- [ ] Esperaste a que descargue el modelo (~20-30 min)
- [ ] Verificaste `/health` con curl o navegador
- [ ] Accediste a http://localhost:8080 (Email AI Dashboard)
- [ ] Subiste documentos en la pesta√±a "Conocimiento"
- [ ] Probaste generando respuestas a correos de prueba

¬°Listo! Ya puedes empezar a usar Email AI con RAG. üöÄ

